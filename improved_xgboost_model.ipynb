{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Churn percentage: 26.54%\n",
      "Churn percentage train:\t26.54%\n",
      "Churn percentage test:\t26.52%\n",
      "\n",
      "[0]\tvalidation_0-aucpr:0.61852\n",
      "Will train until validation_0-aucpr hasn't improved in 10 rounds.\n",
      "[1]\tvalidation_0-aucpr:0.63184\n",
      "[2]\tvalidation_0-aucpr:0.64216\n",
      "[3]\tvalidation_0-aucpr:0.64429\n",
      "[4]\tvalidation_0-aucpr:0.64326\n",
      "[5]\tvalidation_0-aucpr:0.64245\n",
      "[6]\tvalidation_0-aucpr:0.64733\n",
      "[7]\tvalidation_0-aucpr:0.64619\n",
      "[8]\tvalidation_0-aucpr:0.64595\n",
      "[9]\tvalidation_0-aucpr:0.64346\n",
      "[10]\tvalidation_0-aucpr:0.64230\n",
      "[11]\tvalidation_0-aucpr:0.64418\n",
      "[12]\tvalidation_0-aucpr:0.64327\n",
      "[13]\tvalidation_0-aucpr:0.64389\n",
      "[14]\tvalidation_0-aucpr:0.64245\n",
      "[15]\tvalidation_0-aucpr:0.64187\n",
      "[16]\tvalidation_0-aucpr:0.64169\n",
      "Stopping. Best iteration:\n",
      "[6]\tvalidation_0-aucpr:0.64733\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
       "              importance_type='gain', interaction_constraints='',\n",
       "              learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
       "              min_child_weight=1, missing=None, monotone_constraints='()',\n",
       "              n_estimators=100, n_jobs=0, num_parallel_tree=1,\n",
       "              random_state=1337, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
       "              seed=1337, subsample=1, tree_method='exact',\n",
       "              validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import balanced_accuracy_score, roc_auc_score, make_scorer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "\n",
    "\n",
    "seed = 1337\n",
    "df = pd.read_csv('input/telco_customer_churn.csv')\n",
    "\n",
    "# Drop customerID, since it is not related to customer churn.\n",
    "df.drop(['customerID', ], axis=1, inplace=True) # axis=0 for rows, axis=1 for columns\n",
    "\n",
    "# Remove white space in columns for later plotting of tree.\n",
    "df.replace(' ', '_', regex=True, inplace=True)\n",
    "\n",
    "# Look into the dtype == object columns\n",
    "for col in df.select_dtypes(include='object'):\n",
    "    if len(col) < 7:\n",
    "        tabs = '\\t\\t\\t'\n",
    "    elif len(col) < 15:\n",
    "        tabs = '\\t\\t'\n",
    "    else:\n",
    "        tabs = '\\t'\n",
    "    # print(f'{col}:{tabs}{df[col].unique()}')\n",
    "\n",
    "df.loc[(df['TotalCharges'] == '_'), 'TotalCharges'] = 0.0\n",
    "df['TotalCharges'] = pd.to_numeric(df['TotalCharges'])\n",
    "\n",
    "# Let's convert all 'Yes'/'No' columns to 1/0 instead.\n",
    "cols = ['Partner', 'Dependents', 'PhoneService', 'PaperlessBilling', 'Churn']\n",
    "for col in cols:\n",
    "    df[col] = pd.Series(np.where(df[col].values == 'Yes', 1, 0), df.index)\n",
    "    df[col] = pd.to_numeric(df[col])\n",
    "\n",
    "# Convert Male / Female to 1 / 0 similar to above.\n",
    "df['gender'] = pd.Series(np.where(df['gender'].values == 'Male', 1, 0), df.index)\n",
    "df['gender'] = pd.to_numeric(df['gender'])\n",
    "\n",
    "# Divide the data into independent variables X and dependent variable y (Churn).\n",
    "X = df.drop('Churn', axis=1).copy()\n",
    "y = df['Churn'].copy()\n",
    "\n",
    "# Convert to one-hot encoding as this is suitable for trees.\n",
    "# We see that there are a couple of dtype == object columns that are not binary. These, we will one-hot encode.\n",
    "cols = ['MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies', 'Contract', 'PaymentMethod']\n",
    "X_encoded = pd.get_dummies(X, columns=cols)\n",
    "\n",
    "# Check how many Churn == 1 samples there are compared to total.\n",
    "print(f'Churn percentage: {round((sum(y) / len(y) * 100), 2)}%')\n",
    "\n",
    "# Since we only have 26.54% of Churn == 1 samples, we need to use stratification\n",
    "# when splitting out data into a training and a testing dataset. This way, we\n",
    "# ensure that the ratio of 1 / 0 will be the same in the training and test datasets.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_encoded, y, random_state=seed, stratify=y)\n",
    "\n",
    "print(f'Churn percentage train:\\t{round((sum(y_train) / len(y_train) * 100), 2)}%')\n",
    "print(f'Churn percentage test:\\t{round((sum(y_test) / len(y_test) * 100), 2)}%\\n')\n",
    "\n",
    "# In sparse matrices, 0 is used for missing data. I explicitly state missing=None, even\n",
    "# though that is the default. But if our missing data points were '?', we would have said\n",
    "# missing='?'.\n",
    "clf_xgb = xgb.XGBClassifier(objective='binary:logistic', missing=None, seed=seed)\n",
    "\n",
    "# For creating our forest of XGBoosted trees, run fit(). Set early_stopping so we don't\n",
    "# need to wait if the model has stopped improving. Use Area under the PR Curve for evaluating.\n",
    "clf_xgb.fit(X_train,\n",
    "            y_train,\n",
    "            verbose=True,\n",
    "            early_stopping_rounds=10,\n",
    "            eval_metric='aucpr',\n",
    "            eval_set=[(X_test, y_test)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 243 candidates, totalling 729 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=10)]: Using backend LokyBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=10)]: Done  21 tasks      | elapsed:    4.2s\n",
      "[Parallel(n_jobs=10)]: Done 142 tasks      | elapsed:   16.4s\n",
      "[Parallel(n_jobs=10)]: Done 345 tasks      | elapsed:   42.7s\n",
      "[Parallel(n_jobs=10)]: Done 628 tasks      | elapsed:  1.3min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-auc:0.80109\n",
      "Will train until validation_0-auc hasn't improved in 10 rounds.\n",
      "[1]\tvalidation_0-auc:0.81668\n",
      "[2]\tvalidation_0-auc:0.82163\n",
      "[3]\tvalidation_0-auc:0.82501\n",
      "[4]\tvalidation_0-auc:0.82622\n",
      "[5]\tvalidation_0-auc:0.82660\n",
      "[6]\tvalidation_0-auc:0.82591\n",
      "[7]\tvalidation_0-auc:0.83048\n",
      "[8]\tvalidation_0-auc:0.83109\n",
      "[9]\tvalidation_0-auc:0.83326\n",
      "[10]\tvalidation_0-auc:0.83260\n",
      "[11]\tvalidation_0-auc:0.83337\n",
      "[12]\tvalidation_0-auc:0.83443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=10)]: Done 729 out of 729 | elapsed:  1.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13]\tvalidation_0-auc:0.83485\n",
      "[14]\tvalidation_0-auc:0.83493\n",
      "[15]\tvalidation_0-auc:0.83465\n",
      "[16]\tvalidation_0-auc:0.83484\n",
      "[17]\tvalidation_0-auc:0.83481\n",
      "[18]\tvalidation_0-auc:0.83434\n",
      "[19]\tvalidation_0-auc:0.83424\n",
      "[20]\tvalidation_0-auc:0.83397\n",
      "[21]\tvalidation_0-auc:0.83405\n",
      "[22]\tvalidation_0-auc:0.83429\n",
      "[23]\tvalidation_0-auc:0.83437\n",
      "[24]\tvalidation_0-auc:0.83435\n",
      "Stopping. Best iteration:\n",
      "[14]\tvalidation_0-auc:0.83493\n",
      "\n",
      "{'gamma': 0, 'learning_rate': 0.05, 'max_depth': 3, 'reg_lambda': 10.0, 'scale_pos_weight': 3}\n"
     ]
    }
   ],
   "source": [
    "# For imbalanced datasets, the XGBoost manual says that one should balance the positive\n",
    "# and negative weights via scale_pos_weight and that one should use AUC for evaluation.\n",
    "\n",
    "# Use GridSearchCV() for optimizing the hyperparameters. Reduce total optimization time\n",
    "# by dividing the optimization into several rounds.\n",
    "\n",
    "# Optimization round 1\n",
    "param_grid = {\n",
    "    'max_depth': [3, 4, 5],\n",
    "    'learning_rate': [0.1, 0.01, 0.05],\n",
    "    'gamma': [0, 0.25, 1.0],\n",
    "    'reg_lambda': [0, 1.0, 10.0],\n",
    "    'scale_pos_weight': [1, 3, 5]\n",
    "}\n",
    "\n",
    "# Use random 90% subset of the data and a random 50% subset of the features\n",
    "# (i.e. independent variables / columns) per tree in the random forest. This\n",
    "# is for speeding up the cross-validation and for preventing overfitting.\n",
    "optimal_params = GridSearchCV(\n",
    "    estimator=xgb.XGBClassifier(objective='binary:logistic',\n",
    "                                seed=seed,\n",
    "                                subsample=0.9,\n",
    "                                colsample_bytree=0.5),\n",
    "    param_grid=param_grid,\n",
    "    scoring='roc_auc',\n",
    "    verbose=2,\n",
    "    n_jobs=10,\n",
    "    cv=3\n",
    ")\n",
    "\n",
    "optimal_params.fit(X_train,\n",
    "                   y_train,\n",
    "                   early_stopping_rounds=10,\n",
    "                   eval_metric='auc',\n",
    "                   eval_set=[(X_test, y_test)],\n",
    "                   verbose=1)\n",
    "print(optimal_params.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 36 candidates, totalling 108 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=10)]: Using backend LokyBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=10)]: Done  21 tasks      | elapsed:    2.9s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-auc:0.80109\n",
      "Will train until validation_0-auc hasn't improved in 10 rounds.\n",
      "[1]\tvalidation_0-auc:0.81668\n",
      "[2]\tvalidation_0-auc:0.82163\n",
      "[3]\tvalidation_0-auc:0.82501\n",
      "[4]\tvalidation_0-auc:0.82622\n",
      "[5]\tvalidation_0-auc:0.82660\n",
      "[6]\tvalidation_0-auc:0.82591\n",
      "[7]\tvalidation_0-auc:0.83048\n",
      "[8]\tvalidation_0-auc:0.83109\n",
      "[9]\tvalidation_0-auc:0.83326\n",
      "[10]\tvalidation_0-auc:0.83260\n",
      "[11]\tvalidation_0-auc:0.83337\n",
      "[12]\tvalidation_0-auc:0.83443\n",
      "[13]\tvalidation_0-auc:0.83485\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=10)]: Done 108 out of 108 | elapsed:   13.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14]\tvalidation_0-auc:0.83493\n",
      "[15]\tvalidation_0-auc:0.83465\n",
      "[16]\tvalidation_0-auc:0.83484\n",
      "[17]\tvalidation_0-auc:0.83481\n",
      "[18]\tvalidation_0-auc:0.83434\n",
      "[19]\tvalidation_0-auc:0.83424\n",
      "[20]\tvalidation_0-auc:0.83397\n",
      "[21]\tvalidation_0-auc:0.83405\n",
      "[22]\tvalidation_0-auc:0.83429\n",
      "[23]\tvalidation_0-auc:0.83437\n",
      "[24]\tvalidation_0-auc:0.83435\n",
      "Stopping. Best iteration:\n",
      "[14]\tvalidation_0-auc:0.83493\n",
      "\n",
      "{'gamma': 0, 'learning_rate': 0.05, 'max_depth': 3, 'reg_lambda': 10.0, 'scale_pos_weight': 3}\n"
     ]
    }
   ],
   "source": [
    "# Round 1 yielded:\n",
    "# {'gamma': 0, 'learning_rate': 0.05, 'max_depth': 3, 'reg_lambda': 10.0, 'scale_pos_weight': 3}\n",
    "\n",
    "# Optimization round 2 - building on results of optimization round 1.\n",
    "param_grid = {\n",
    "    'max_depth': [3, 4],\n",
    "    'learning_rate': [0.05, 0.1, 0.5],\n",
    "    'gamma': [0, 0.25],\n",
    "    'reg_lambda': [10.0, 20.0, 100],\n",
    "    'scale_pos_weight': [3]\n",
    "}\n",
    "\n",
    "optimal_params = GridSearchCV(\n",
    "    estimator=xgb.XGBClassifier(objective='binary:logistic',\n",
    "                                seed=seed,\n",
    "                                subsample=0.9,\n",
    "                                colsample_bytree=0.5),\n",
    "    param_grid=param_grid,\n",
    "    scoring='roc_auc',\n",
    "    verbose=2,\n",
    "    n_jobs=10,\n",
    "    cv=3\n",
    ")\n",
    "\n",
    "optimal_params.fit(X_train,\n",
    "                   y_train,\n",
    "                   early_stopping_rounds=10,\n",
    "                   eval_metric='auc',\n",
    "                   eval_set=[(X_test, y_test)],\n",
    "                   verbose=1)\n",
    "print(optimal_params.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# {'gamma': 0, 'learning_rate': 0.05, 'max_depth': 3, 'reg_lambda': 10.0, 'scale_pos_weight': 3}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
